\documentclass[12pt,twoside]{mitthesis-exec}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE

\usepackage[bitstream-charter]{mathdesign} % Use BT Charter font
\usepackage[T1]{fontenc}                   % Use T1 encoding instead of OT1
\usepackage[utf8]{inputenc}                % Use UTF8 input encoding
\usepackage{microtype}                     % Improve typography
\usepackage{amsmath}                       % AMS Math extensions
\usepackage{booktabs}                      % Improve table spacing
\usepackage{graphicx}                      % Extended graphics capabilities
\usepackage{tocbibind}                     % Include listings in TOC
\usepackage[printonlyused]{acronym} % withpage: for showing page of use
\usepackage{listings}                      % Source code listings
\usepackage{caption}
\usepackage{subcaption}
\usepackage[rgb]{xcolor}
\usepackage{url}
\usepackage{soul}
\usepackage{array}
\usepackage{pdfpages}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage{pbox}
\usepackage{tikz}
\usetikzlibrary{calc,shapes,decorations.pathreplacing,positioning}
\usepackage{pgfplots}

\usepackage[breaklinks=true]{hyperref}
\hypersetup{colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black,
  pdftitle={Reactor Agnostic Multi-Group Cross Section Generation for Fine Mesh Deterministic Neutron Transport Simulations},
  pdfauthor={William Robert Dawson Boyd}
}
\pagestyle{plain}

%\usepackage{floatrow}
%\floatsetup[table]{style=plaintop}
%\floatsetup[widefigure]{margins=hangleft}

% Appendix
\usepackage[toc,page]{appendix}

% Don't reset footnote counter between chapters
\usepackage{chngcntr}
\counterwithout{footnote}{chapter}

% Algorithm constructs
\usepackage[chapter]{algorithm} % Provides algorithm environment
\usepackage{algorithmicx}       % Provides algorithmic block
\usepackage{algpseudocode}      % Option of algorithmicx package
\renewcommand{\thealgorithm}{\thechapter-\arabic{algorithm}}
\newcommand\Algphase[1]{%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\columnwidth}{0.4pt}%
\Statex\hspace*{-\algorithmicindent}{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\columnwidth}{0.4pt}%
}
\newcommand{\algrule}[1][.4pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}

% Configure captions
\captionsetup{labelfont=bf, labelsep=colon}
\captionsetup[algorithm]{labelfont=bf, labelsep=colon}

% Use Latin Modern for typewriter fonts
\renewcommand{\ttdefault}{lmtt}

% Add \unit macro
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
\lstset{
  basicstyle=\footnotesize\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape,
  frame=single,
  xleftmargin=0.55in
}

\lstdefinelanguage{XML}
{
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  morecomment=[s]{<!--}{-->},
  stringstyle=\color{black},
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  morekeywords={}
}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}

\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\bibname}{References}

\makeatletter \renewcommand\thealgorithm{\arabic{algorithm}} \@addtoreset{algorithm}{chapter} \makeatother

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE PAGE

\title{EXECUTIVE SUMMARY \\~\\ Reactor Agnostic Multi-Group Cross Section Generation for Fine Mesh Deterministic Neutron Transport Simulations}

\author{William Robert Dawson Boyd III}
\prevdegrees{B.S., Georgia Institute of Technology (2010) \\
             M.S., Massachusetts Institute of Technology (2014)}
\department{Department of Nuclear Science and Engineering}
\degree{Doctor of Philosophy in Nuclear Science and Engineering}

\degreemonth{February}
\degreeyear{2016}
\thesisdate{November 4, 2016}

\supervisor{Benoit Forget}{Associate Professor of Nuclear Science and Engineering}
\reader{Kord S. Smith}{KEPCO Professor of the Practice of Nuclear Science and Engineering}
\chairman{Emilio Bagglietto}{Associate Professor of Nuclear Science and Engineering}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT

\setcounter{savepage}{\thepage}

\include{frontmatter/abstract}

\singlespacing 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Background and Motivation}

We desire higher-fidelity simulations of full nuclear reactor systems to
help us better understand the current fleet of aging reactors, as well as better
evaluate new designs. One option for solving for neutron distributions is with
\textbf{Monte Carlo} (MC) methods.

\begin{itemize}
  \item MC methods provide the highest-quality solutions by sampling directly from
  evaluated nuclear data to carry out random walks for individual particles
  through an arbitrarily complex geometry with minimal approximations.
  \item MC particle histories are independent within a fission generation, making
  the method ``embarrassingly parallel.''
  \item Immense computational effort is needed to achieve good statistical
  convergence for reactor problems.
\end{itemize}

Such methods have typically only been thought viable for either small reactors
and experiments or for selective verification of coarse-grid deterministic
methods, which are used for routine design and analysis.

To achieve the increased level of fidelity desired it is clear that
deterministic methods require considerable refinement of the phase space discretization.
For instance, Smith's recently-updated challenge and the accompanying BEAVRS benchmark
call for over 50M regions at a minimum, each of which contain a material with
over 300 isotopes that must be depleted independently. In this case, it is clear
that high performance computing (HPC) systems will be required, and methods must
be able to scale to many tens of thousands or millions of processors.

In practice this can be quite difficult for existing methods, especially since
the amount of memory available per process is continuing to shrink in modern
computing systems. In fact, we have come to a cross-over point where the
amount of computational effort required to solve reactor systems
deterministically is similar to that required for a Monte Carlo simulation to
converge to a comparable level of accuracy. This, along with inherent
parallelism, makes Monte Carlo a particularly attractive choice
for ultra-high fidelity reactor core neutronics simulations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection*{Summary of Issues}

There are several challenges to overcome before Monte Carlo can be used for
high-fidelity simulations, which generally fall into the following categories:

\begin{enumerate}
  \item Time to solution
  \begin{itemize}
    \item[a] High dominance ratios make the fission source site distribution
        difficult to converge with power iteration.
    \item[b] Many histories are required for tally statistics to converge to a
        reasonable accuracy.
  \end{itemize}
  \item Memory
  \begin{itemize}
    \item[a] Hundreds of gigabytes are needed for cross section data for temperature treatment.
    \item[b] Several terabytes are needed for reaction rate tallies and material
        composition data for depletion.
  \end{itemize}
\end{enumerate}

The first issue has long been recognized as major disadvantage of using Monte
Carlo for reactor problems, where the vast number of particles that must be
tracked would require an excessive amount of computation time. Both facets of
this issue have received considerable attention in the literature, and can be
addressed with various variance reduction techniques as well as hybrid
source-convergence acceleration scheme. Regardless, billions or trillions are
particles are still required for fine tally meshes.

This thesis is primarily concerned with adapting the Monte Carlo algorithm
for high-fidelity runs on HPC architectures, which if done correctly can help
alleviate most of this issue given the inherent parallelism of particle history
tracking. Thus, it is not specifically explored in detail in this work.

Likewise, the cross section memory problem has also seen excellent recent
progress in the literature. For instance, rather than using many
Doppler-broadened libraries on the order of gigabytes in size for interpolation
between temperature points, it is likely that techniques such as on-the-fly
broadening or explicit treatment of thermal motioncan drastically reduce this
memory burden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Tally and Material Memory}

This thesis is primarily concerned with the practical difficulties of
accommodating the discretization needed to accomplish multi-physics coupling to
depletion, thermal-hydraulics, and fuel performance routines. This issue
primarily manifests in enormous memory requirements for tallies and materials,
on the order of terabytes for realistic problems such as BEAVRS.

While this is not a large amount of aggregate memory on HPC machines, the
traditional parallel model for Monte Carlo requires that all data be replicated
on each distributed compute node - a clearly untenable situation given the
ever-shrinking amount of memory available per compute core.

The problem can be easily quantified with an example. For instance, if the
50,952 fuel pins in the BEAVRS geometry are treated with 10 radial rings and 100
axial sections as prescribed by Smith and Forget, the
model will have almost 51 million fuel regions, each of which contain a
material with roughly 300 nuclides. Since each region is to be depleted
separately, requiring up to 6 reaction rate tallies per nuclide, we see that:

\begin{itemize}
  \item Material isotope atom densities: (51M regions) $\times$ (300 nuclides)
  $\times$ (8 bytes per density) = 122,284,800,000 bytes = or \textbf{114
  gigabytes}
  \item Depletion tallies: (51M regions) $\times$ (300 nuclides) $\times$ (6
  tallies) $\times$ (24 bytes per tally\footnote{Three real numbers are required per tally for MC methods that use batch statistics: a batch accumulator, and accumulators for the mean and variance.}) = 2,201,126,400,000 bytes = \textbf{2
  terabytes}
\end{itemize}

These requirements are expected to grow higher with further refinements in
 depletion mesh and increased availability of nuclear data libraries for the
hundreds of other nuclides in the fuel. If this burden is to be accommodated,
clearly the data must be spread across a distributed set compute nodes. This can
be done either with \emph{data decomposition} or \emph{domain decomposition}.

With data decomposition, a number of nodes are dedicated as ``memory servers''
for particle tracking nodes to access quantities over the network as needed.
This has been explored by Romano for tally data nd by Walsh
\cite{walsh_ms_thesis} for cross section data, with performance models and
simple demonstrations predicting that full-scale implementations could solve
this issue without prohibitive overhead. These approaches add significant
complexity to the algorithm and still need to be demonstrated at full-scale, but
they do show promise.

Alternatively, domain decomposition holds the potential to solve both the
material and tally memory problems by dividing the spatial domain among particle
tracking nodes. In this approach, nodes would only allocate
memory for tallies and materials that are relevant to the domain they are
assigned to, and then communicate particles to one another if sampled
trajectories would take them out of the domain in which they are being tracked.

Considerably more effort has been expended exploring domain decomposition for
Monte Carlo than for data decomposition, but to-date, the full scope
problem has not been attempted due to challenges
inherent in implementation. This thesis focuses on overcoming these
difficulties.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Domain Decomposition}

\underline{Particle communication costs} have been cited as a prohibitive
drawback of using domain decomposition for Monte Carlo particle transport as far
back as the first time it was described by Alme et al. \cite{js-alme-2001} in
2001. This is echoed throughout the literature, for example in two prominent
review papers by Brown in 2011 \cite{forrest_mc_prospects} and Martin in 2012
\cite{martin_chall}. By the conventional wisdom, the extremely large number of
histories that need to be tracked will create intolerable network logjams
between distributed compute nodes.

This was challenged by Siegel et al. in 2012 \cite{Siegel1} with the
development of a performance model that uses network parameters of current
HPC machines. They demonstrated the validity of the model with a simple toy
code, which indicated that network performance is likely not a limiting factor.

Another oft-cited drawback of domain decomposition is the potential for
\ul{load imbalances} and the parallel inefficiencies it would cause.
This was also explored by Siegel et al. in 2013 \cite{Siegel2} with another
performance model, which related network properties, processor speeds, and
domain leakage rates to predict an upper bound on such inefficiencies. Using
data from an approximated PWR benchmark they concluded that the load imbalance
problem should not be as bad as previously-thought, but still predicted that
significant overhead would be observed for finer domain meshes.

Finally, another perhaps more mundane challenge with a true implementation of
domain decomposition stems from the \underline{data management} that must take
place. For example, for arbitrary domain meshes and arbitrary constructive solid geometries (CSG) it
is not immediately straightforward to determine, or computationally cheap, to
calculate which tally regions should be tracked by which processors. In
addition, a significant amount of bookkeeping is required to keep track of which
tally bins are allocated where, since domains will not necessarily cut tally
arrays into contiguous chunks. This also complicates tally reduction algorithms,
which must determine which processors and which sections of memory to accumulate
scores with. Furthermore, check-pointing (\emph{i.e.}, saving the entire simulation
state for future restarts) and finalization I/O is no longer an
insignificant task at the scale of terabytes. All of these things must be done
efficiently in order for a large-scale run to be achievable with domain
decomposition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Objectives}

The over-arching objective of this thesis is to advance Monte Carlo neutron
transport methods towards carrying out high-fidelity nuclear reactor
analyses with the full complement of tallies needed for multi-physics coupling
and depletion. With memory constraints identified as one of the key limiting
factors, a more detailed exploration into domain decomposition has been pursued. The 
major objectives of this thesis are:

\begin{enumerate}
  \item Implement a robust domain decomposition scheme into the full-physics code:
  OpenMC
  \item Explore and confirm network communication burdens and load imbalance
  penalty predictions for full scale problems
  \item Explore load balancing techniques to mitigate parallel inefficiencies
  \item Explore optimizations in internal handling of data structures and data
  management required for efficiency and scalability during simulation and I/O
  \item Demonstrate the viability of carrying out simulations using the full
  tally memory requirements
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Domain Decomposition Implementation}

Chapter 2 of this thesis details the domain decomposition implementation added
to OpenMC, as well as several modifications to support the large-scale memory
burden of full-core PWR simulations. \ul{Key features} included:

\begin{itemize}
  \item Domains are defined with an arbitrary mesh of rectangular cuboids, overlaid on an
  arbitrary geometry.
  \item Material and tally regions can be defined without specifying which domains they should be loaded on.
  \item Simple universe-based problem descriptions can be used without
  duplicating data structures for millions of regions.
  \item Additional compute resources can be distributed across domains according
  to the work distribution in order to address load imbalances.
  \item Random number reproducibility is maintained regardless of the number of
  domains and/or processes that are used.
\end {itemize}

This implementation also has several \ul{key limitations}:

\begin{itemize}
  \item Tally cells must reside in one domain each (\emph{i.e.}, cells may not
  be cut by domain boundaries if tallies are applied to them).
  \item Efficient material and tally handling relies on the use of a new
  distributed cell mechanism that complicates post-processing.
  \item Effective load balancing requires knowledge of particle work
  distributions.
  \item Each processor can only track particles on one domain.
\end {itemize}

Subsequent chapters explore the performance of this implementation, both from a
communication and load balancing perspective as well as overall ability to
tackle the full-scale problem of interest.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Performance Model}

Chapter 3 of this thesis explores the inter-domain particle communication and
load imbalance costs that arise when using domain decomposition. The theoretical
framework established by Romano and Siegel et al. for understanding these is
relevant to implementation pursued in this thesis \cite{Siegel1,
Siegel2, romano_thesis}. However, the previous predictions can be improved upon if some of the
simplifications to the model are relaxed. Importantly, the upper-bound load
imbalance penalty predictions of \cite{Siegel2} are most-likely severely
over-estimated.

In this work the performance model is re-derived to a different form, where
parameters in the final equations are arranged in a way that is more amenable to
making direct predictions of load imbalance penalties from tallied domain
leakage data without resorting to an upper-bound only. The updated model is
verified with tallied particle movement data and compared with run times using
the new DD implementation.

As described in \cite{Siegel2}, the time $\tau$ needed to complete a perfectly
load-balanced domain-decomposed Monte Carlo run using this simple scheme can be
modeled as a combination of latency, bandwidth, and particle tracking
components:

\begin{equation}
    \tau = \tau_{latency} + \tau_{bandwidth} + \tau_{tracking}
    \label{eqn:baltime_high-level}
\end{equation}

It is straightforward to write this in terms of the number of synchronization
stages $M$, the average number of particles $\bar P_i$ run in each domain at
stage $i$, and the average fraction $\bar \lambda_i$ of particles that leak out
of each domain at stage $i$:

\begin{equation}
    \tau = 6\alpha M + \beta \sum_{i=0}^{M-1} \bar\lambda_i \bar{P}_i + \mu \sum_{i=0}^{M-1} \bar{P}_i
    \label{eqn:baltime}
\end{equation}

\noindent where $\alpha$, $\beta$, and $\mu$ are the latency, inverse bandwidth,
and inverse particle tracking rate coefficients of the system, with units of
seconds per connection, seconds per particle transferred, and seconds per
particle simulated, respectively. This applies to a rectilinear decomposition
where each domain has six Cartesian neighbors.

For real problems with load imbalances, all processes must wait at the
synchronization points between stages, meaning the time in this case is 
determined by the maximum load, rather than the average.  Thus, $\tau'$
for the load-imbalanced problem can be written as:

\begin{equation}
    \begin{aligned}
    \tau' &= 6 \alpha M + 
      \beta \sum_{i=0}^{M-1} \left(\lambda_i p_i\right)^{max} +
      \mu \sum_{i=0}^{M-1} p_i^{max}
    \end{aligned}
    \label{eqn:imbaltime}
\end{equation}

\noindent where $p_i^{max}$ and $\left(\lambda_i p_i\right)^{max}$ denote the
load and leakage for the domains at stage $i$ that take the longest to finish
tracking and transmitting particles, respectively (these could be from the same
domain, but not necessarily). We can then quantify the magnitude of the
\emph{load imbalance penalty} $\Delta$ caused by the blocking synchronization
points between stages as

\begin{equation}
    \Delta \equiv \frac{\tau' - \tau}{\tau}.
    \label{eqn:penalty}
\end{equation}

In \cite{Siegel2} the authors took this a step further, expanding the sums in
Equations \ref{eqn:baltime} and \ref{eqn:imbaltime} to write them in terms of
the load imbalance distribution at the initial particle tracking stage. However,
in this thesis these equations are used directly to arrive at more accurate
predictions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY

\begin{singlespace}
\bibliographystyle{ans}
\bibliography{references}
\end{singlespace}

\end{document}
