\documentclass[12pt,twoside]{mitthesis-exec}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE

\usepackage[bitstream-charter]{mathdesign} % Use BT Charter font
\usepackage[T1]{fontenc}                   % Use T1 encoding instead of OT1
\usepackage[utf8]{inputenc}                % Use UTF8 input encoding
\usepackage{microtype}                     % Improve typography
\usepackage{amsmath}                       % AMS Math extensions
\usepackage{booktabs}                      % Improve table spacing
\usepackage{graphicx}                      % Extended graphics capabilities
\usepackage{tocbibind}                     % Include listings in TOC
\usepackage[printonlyused]{acronym} % withpage: for showing page of use
\usepackage{listings}                      % Source code listings
\usepackage{caption}
\usepackage{subcaption}
\usepackage[rgb,table]{xcolor}
\usepackage{url}
\usepackage{soul}
\usepackage{array}
\usepackage{pdfpages}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage{pbox}
\usepackage{tikz}
\usetikzlibrary{calc,shapes,decorations.pathreplacing,positioning}
\usepackage{pgfplots}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{breqn}

% Specialties for tables
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage[breaklinks=true]{hyperref}
\hypersetup{colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black,
  pdftitle={Full Core Simulation of the 3D Neutron Transport Equation Using the Method of Characteristics with Linear Sources},
  pdfauthor={Geoffrey Alexander Gunow}
}
\pagestyle{plain}

%\usepackage{floatrow}
%\floatsetup[table]{style=plaintop}
%\floatsetup[widefigure]{margins=hangleft}

% Highlights and emphasis boxes from Bryans thesis
\usepackage[framemethod=tikz]{mdframed}
\definecolor{mitred}{rgb}{0.698,0.0314,0.216}
\definecolor{mitgray}{rgb}{0.690,0.694,0.710}
\definecolor{canyellow}{rgb}{0.933, 0.965, 0.424}
\newmdenv[nobreak=false, skipabove=2ex, skipbelow=2ex, innerlinewidth=3pt, innerlinecolor=black, backgroundcolor=mitgray!75, roundcorner=10pt, frametitlerule=true, frametitlerulewidth=2.5pt, frametitlefont=\color{white}\Large\bfseries, frametitlealignment=\centering, frametitlebackgroundcolor=mitred, frametitleaboveskip=2ex, frametitlebelowskip=2ex, innertopmargin=3ex, innerbottommargin=2ex]{highlightsbox}
\newmdenv[nobreak=false, skipabove=2ex, skipbelow=2ex, innerlinewidth=2pt, innerlinecolor=black, backgroundcolor=white, roundcorner=10pt]{emphbox}

% Appendix
\usepackage[toc,page]{appendix}

% Don't reset footnote counter between chapters
\usepackage{chngcntr}
\counterwithout{footnote}{chapter}

% Algorithm constructs
\usepackage[chapter]{algorithm} % Provides algorithm environment
\usepackage{algorithmicx}       % Provides algorithmic block
\usepackage{algpseudocode}      % Option of algorithmicx package
\renewcommand{\thealgorithm}{\thechapter-\arabic{algorithm}}
\newcommand\Algphase[1]{%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\columnwidth}{0.4pt}%
\Statex\hspace*{-\algorithmicindent}{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\columnwidth}{0.4pt}%
}
\newcommand{\algrule}[1][.4pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}

% Configure captions
\captionsetup{labelfont=bf, labelsep=colon}
\captionsetup[algorithm]{labelfont=bf, labelsep=colon}

% Use Latin Modern for typewriter fonts
\renewcommand{\ttdefault}{lmtt}

% Add \unit macro
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
\lstset{
  basicstyle=\footnotesize\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape,
  frame=single,
  xleftmargin=0.55in
}

\lstdefinelanguage{XML}
{
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  morecomment=[s]{<!--}{-->},
  stringstyle=\color{black},
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  morekeywords={}
}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}

\renewcommand{\contentsname}{Table of Contents}
\renewcommand{\bibname}{References}

\makeatletter \renewcommand\thealgorithm{\arabic{algorithm}} \@addtoreset{algorithm}{chapter} \makeatother

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE PAGE

\include{frontmatter/exec_cover}

\title{EXECUTIVE SUMMARY \\~\\ Full Core Simulation of the 3D Neutron Transport Equation Using the Method of Characteristics with Linear Sources}

\author{Geoffrey A. Gunow}
\prevdegrees{B.S.E., University of Michigan (2012) \\
	M.S., Massachusetts Institute of Technology (2015)}
\department{Department of Nuclear Science and Engineering}
\degree{Doctor of Philosophy in Computational Nuclear Engineering}

\degreemonth{June}
\degreeyear{2018}
\thesisdate{March 1, 2018}

%\supervisor{Benoit Forget}{Associate Professor of Nuclear Science and Engineering}
%\reader{Kord S. Smith}{KEPCO Professor of the Practice of Nuclear Science and Engineering}
%\chairman{Emilio Bagglietto}{Associate Professor of Nuclear Science and Engineering}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT

%\setcounter{savepage}{\thepage}

\include{frontmatter/exec_summ_abstract}

\singlespacing 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Background and Motivation}

Traditional simulation tools in modern reactor analysis are typically based on solvers which incorporate nodal diffusion methods. While these solvers have been sufficiently accurate for simulating currently operating nuclear reactors, advanced reactor designs pose significant challenges for nodal methods. Specifically, nodal diffusion solvers often assume smooth axial variation of scalar flux distributions within reactor cores. However, advanced reactor designs, such as the Westinghouse AP1000 Pressurized Water Reactor (PWR), incorporate significant axial detail including partially inserted control rods, axial enrichment zoning, and partial length burnable poisons. All of these features lead to axial profiles which are not smooth enough for modern nodal methods to be sufficiently accurate.

% Traditional -> only methods used in production
 
Alternatively, solvers based on neutron transport fundamentals can be implemented which do not rely on the assumptions and approximations present in nodal diffusion methods. While these solvers lead to a dramatic increased in computational requirements for reasonable solutions, recent advances in computational power for both engineering clusters and large scale supercomputers have enabled the computation of extremely large scale reactor simulations.

%MOC has often been used for 2D lattice physics calculations on single assemblies. In traditional simulation of nuclear reactors, a lattice physics calculation for each unique assembly is conducted in isolation on each single assembly throughout the .  on single assemblies which are then coupled with a nodal diffusion simulator to achieve 3D solutions.  Rather than following this traditional approach, this thesis concentrates on the direct simulation 

One method which is based on neutron transport fundamentals is the Method of Characteristics (MOC), which discretizes the neutron transport equation using many characteristic paths or directions which traverse the geometry. This leads to a system of equations which is usually solved with the \textit{source iteration} process in which successive iterations estimate the neutron source in each region, leading to new estimates of scalar fluxes. Since the convergence of source iteration is typically quite slow, Coarse Mesh Finite Difference (CMFD) acceleration is often used to converge in a reasonable number of iterations.

MOC has seen widespread use for 2D lattice physics analysis of single assembly models in which the axial direction is assumed to be infinite. The full 3D detail is then formed by coupling with a nodal diffusion solver. Instead of following this tradition simulation scheme, this thesis concentrates on the direct 3D MOC simulation of reactor physics problems, incorporating all 3D geometric detail. Previously, others have attempted to simulate reactor physics problems using 3D MOC but have been limited to small models due to computational constraints of the particular 3D MOC implementations~\cite{kochunas}.

\subsection*{Objectives}

This thesis concentrates on developing an efficient 3D MOC solver which can solve large reactor physics problems. The behavior of 3D MOC and the sensitivity of its solution accuracy to input parameters, such as mesh refinement, is studied on a variety of realistic reactor physics problems. However, \textbf{the primary goal of this thesis is to directly use 3D MOC to accurately simulate the BEAVRS benchmark}, which represents a full core PWR.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Implementation}

%%%%%%%%%%%%%%%%%%%%
\subsection*{Software Design}

OpenMOC is an open source neutron transport solver which was originally designed for only 2D MOC calculations. Like many MOC solvers, OpenMOC solves the MOC equations using source iteration with CMFD acceleration. In this thesis, OpenMOC is extended to include a 3D MOC solver which maintaining the 2D MOC capabilities. In addition, the user input for 2D and 3D simulations was designed to be very similar, allowing a user to switch between either mode rather easily on the same model. Accomplishing this presented significant difficulties in the OpenMOC code design. Therefore, the underlying code structure was significantly altered to become more flexible and modular while maintaining efficient memory layout, minimizing memory allocations, and optimizing cache efficiency. 

The new modular structure allows for greater reuse of code, leading to more robust code which minimizes the potential for bugs. In addition, the modular structure allows for more robust comparison of the internal algorithms. For instance, all MOC codes require a ray tracing algorithm. In OpenMOC, a variety of ray tracing schemes are implemented. With the incorporation of the modular design, simulations using different ray tracing algorithms can be easily compared with all other code remaining unchanged, removing variables from trials and leading to increased confidence in the results. 

%%%%%%%%%%%%%%%%%%%%%
\subsection*{Track Laydown}

In MOC algorithms, tracks are laid across the geometry, each representing a particular direction. The MOC equations are then applied along each track. One of the often overlooked aspects of MOC implementations is the method chosen to lay down tracks across the geometry. In order to accommodate periodic and reflective boundary conditions, tracks are often laid down such that they form cycles over the geometry. This allows for direct treatment of boundary conditions. MOC ray parameters are adjusted so that tracks link at boundaries. Figure~\ref{fig:sample-tracks} depicts a coarse cyclic 3D track laydown.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{figures/laydown/sample-tracks_b.png}
	\caption{Illustration of a coarse 3D cyclic track laydown}
	\label{fig:sample-tracks}
\end{figure}

There are a variety of ways cyclic tracking can be enforced by adjusting MOC ray parameters. Depending on the track laydown algorithm, the parameter adjustment can be significant, often inserting far more tracks than necessary. Inefficient track laydown algorithms can lead to an order of magnitude increase in the number of tracks required for realistic PWR problems~\cite{shaner-laydown}. Since the computational cost of MOC scales directly with the number of tracks, an order of magnitude increase in the number of tracks translates directly into an order of magnitude increase in the run time. OpenMOC implements track laydown using Modular Ray Tracing (MRT), which has quite flexible constraints with a minimal number of additional track insertions~\cite{liu_mrt}.


%%%%%%%%%%%%%%%%%%%%
\subsection*{On-the-fly Ray Tracing}

After tracks are laid across the geometry, each track is discretized by mesh boundaries into \textit{segments} using a ray tracing algorithm. MOC equations are applied directly to segments. The algorithm used to retrieve track ray tracing information can have a significant impact on computational performance. Traditional MOC implementations conduct ray tracing upfront, store the associated ray tracing data, and reference it during the solver. While this approach is straightforward, its memory and compute requirements for 3D MOC can be prohibitive, even for small problems, due to the vast number of segments present in 3D MOC simulations~\cite{physor2016otf}. Reducing the memory footprint is important for many reasons including improved cache efficiency and reducing bulk memory requirements.

In this thesis, an alternative approach is presented that greatly reduces the segment storage and generation requirements by taking advantage of the extruded geometry structure common to many reactor physics problems. This alternative approach saves no 3D segment data, rather treating the ray tracing problem as a coupled 2D and 1D system whereby 2D radial ray tracing information is combined with 1D axial information to compute the 3D intersections~\cite{physor2016otf}.

Two on-the-fly ray tracing approaches are introduced in this thesis. One approach ray traces each 3D track individually. Another ray traces an entire grouping of tracks stacked in the axial direction together. This group of tracks is referred to as a $z$-stack. Both approaches offer significant memory reduction with minimal or no computational overhead.

%%%%%%%%%%%%%%%%%%%%
\subsection*{Linear Source Approximation}

A flat source approximation is typically used in MOC implementations whereby the total source in each mesh cell is approximated by a constant. While this approximation is convenient and used by many in practice~\cite{kochunas, dragon_3d_moc, apollo3_vv, cactus_3d, liu_mrt, mockingbird}, a linear approximation can potentially reduce the computational requirements of simulating a fully converged reactor physics problem. While the linear source approximation increases the computational cost for a fixed discretization, the higher order source can capture source gradients, allowing for a much coarser mesh discretization while maintaining solution accuracy~\cite{ferrer2012linear}. In this thesis, the track-based linear source approximation introduced by Ferrer~\cite{ferrer2015linear} for 2D MOC is extended to 3D MOC and implemented in OpenMOC.

To evaluate on-node performance, the Single Domain Single Assembly (SDSA) test problem is created which is a nearly cubic cutout of the BEAVRS reactor. Specifically, the test problem represents a single assembly radially that has been extruded 20 cm in height with reflective boundaries placed on the top and bottom. It is meshed with the discretization expected to accurately simulate the BEAVRS benchmark. The size of the cutout was chosen to be the largest cutout to reasonably fit on one node of the Argonne BlueGene/Q supercomputer.

Singe-thread performance is often useful for determining the efficiency of an MOC implementation. One useful performance metric is the integration time refers to the time required to compute the angular flux variation over a segment and tally its contribution to the local scalar flux for a single energy group. Since the number of segments increases with the size of the problem, integration time is less problem-dependent than other performance metrics such as run-time. Single thread computational results of both on-the-fly ray tracing schemes are presented in Table~\ref{tab:rt-single-thread} for the SDSA test problem with fixed mesh.

\begin{table}[ht]
	\centering
	\caption{Single thread performance of OpenMOC using one node of the Falcon supercomputer}
	\medskip
	\begin{tabular}{l|l|l}
		\hline
		Ray Tracing Scheme & Source Approximation & Integration Time (ns) \\
		\hline
		By Track  & Flat & 12.2 +/- 0.01 \\
		By $z$-Stack & Flat & 10.8 +/- 0.01 \\
		\hline
		By Track  & Linear & 31.41 +/- 0.08 \\
		By $z$-Stack & Linear & 29.87 +/- 0.03 \\
		\hline
	\end{tabular}
	\label{tab:rt-single-thread}
\end{table}

Both ray tracing methods are able to achieve $\approx 10$ ns per integration with the flat source approximation. The linear source approximation adds a $\approx 2 $ -- $3 \times$ overhead. However, the higher order source approximation allows the mesh to be significantly coarsened while maintaining solution accuracy. For both flat and linear source solvers, on-the-fly ray tracing by $z$-stack slightly outperforms on-the-fly ray tracing by individual 3D track. 

It is important to parallelize algorithms for any large application. OpenMOC uses OpenMP shared memory parallelism~\cite{openmp} on each node. The scaling results of the linear source solver using ray tracing by $z$-stack are shown in Figure~\ref{fig:rt-parallel-ls-cetus} using the Cetus partition of the Argonne BlueGene/Q supercomputer to solve the SDSA test problem. This machine has 16 CPU cores per node and hyper-threads allow for improved resource utilization with up to 64 threads. It is important to note that beyond 16 threads, ideal speedup is not expected as threads must share the computation resources of the 16 cores. At 16 threads, the speedup is 90.9\% of ideal.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/results/performance/ls-parallel-scaling-stacks-cetus.png}
	\caption{Strong scaling performance of the linear source solver on the SDSA test problem using the Cetus partition of the Argonne BlueGene/Q supercomputer with on-the-fly ray tracing by $z$-stacks.}
	\label{fig:rt-parallel-ls-cetus}
\end{figure}


\newpage
%%%%%%%%%%%%%%%%%%%%
\subsection*{Domain Decomposition}

Solvers need to be able to scale to many computational nodes in order to solve large reactor physics problems, such as the BEAVRS benchmark. While shared memory parallelism is feasible for on-node scaling, it is infeasible for inter-node scaling due to significant latency when transferring information between nodes. Therefore, OpenMOC adopts a hybrid parallelism model in which on-node scaling is handled with OpenMP shared memory parallelism and inter-node scaling is handled with spatial domain decomposition using MPI~\cite{mpi}.

OpenMOC implements spatial domain decomposition by partitioning the geometry using a uniform grid into many geometrical sub-domains. Each sub-domain represents an encapsulated MOC problem, only relying on other sub-domains for angular fluxes and a few global variables such as normalization and the eigenvalue. To communicate angular flux information, the same modular track laydown is used on each sub-domain in which tracks naturally link at boundaries. By having a modular track laydown~\cite{liu_mrt}, each sub-domain can easily calculated connecting track indexes with periodic track indexes.

Numerous scaling studies were conducted to ensure efficient parallel scaling of the domain decomposition implementation to many nodes. One such study replicated the SDSA test problem in a cubic 3D lattice. Weak scaling tests were conducted on the Cetus partition of the Argonne BlueGene/Q supercomputer with each node assigned one of the lattice cells (equivalent to one SDSA geometry). Therefore problem size on each node kept constant as the $N \times N \times N$ lattice is expanded. The results are presented in Figure~\ref{fig:dd-ws-3D}, showing greater than 90\% parallel efficiency for all configurations. The largest case involves a $12 \times 12 \times 12$ lattice consuming 1728 nodes (27,648 cores), resulting in an efficiency of 92\%.
	
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/DD/dd-ws-3D.png}
	\caption[]{Weak scaling inter-node parallel efficiency of the domain decomposition implementation of the linear source solver on a replicated 3D lattice of the SDSA test problem.}
	\label{fig:dd-ws-3D}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Convergence of Source Iteration}

The OpenMOC solver described in this thesis relies on source iteration used in conjunction with CMFD acceleration to solve the MOC equations. While convergence of source iteration is straightforward for physical cross-sections, the use of transport-corrected cross-sections can cause convergence issues. Transport correction is required to produce accurate solutions using OpenMOC due to an isotropic source assumption.

This thesis introduces a novel strategy to overcome the convergence issues of source iteration using damping of the MOC scalar fluxes, termed diagonal stabilization. A robust description of the source of the cause of convergence issues with transport-corrected cross-sections is discussed in this thesis. Diagonal stabilization is tested on multiple reactor physics problems, including the full core BEAVRS model, showing it is necessary to achieve convergence using a reduced CMFD group structure. A comparison of the convergence behavior with and without diagonal stabilization is shown in Figure~\ref{fig:fc-3D-ls} for the BEAVRS full core model using the OpenMOC linear source solver with coarse MOC ray parameters, showing an inability to converge the problem without diagonal stabilization.
	
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/convergence/full-core-3D-ls.png}
	\caption{Convergence behavior of OpenMOC's linear source solver on the full core BEAVRS benchmark with and without Diagonal Stabilization.}
	\label{fig:fc-3D-ls}
\end{figure}

It is important to note that this stabilization strategy has implications for all neutron transport solvers -- not just MOC solvers. Any solver which relies on source iteration has the potential to experience these same convergence issues when using transport-corrected cross-sections. Due to its wide-ranging applicability, this diagonal stabilization method is one of the most important contributions of this thesis to the reactor physics community. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Simulation Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Single Assembly Sensitivity Studies}

After the OpenMOC solver has been fully implemented, sensitivity to the MOC parameters can be tested. A variety of cutouts from the BEAVRS model are selected to test the sensitivity to MOC ray and mesh parameters. The tested models include a 2D extruded BEAVRS core in which a thin axial region is simulated with axially reflective boundary conditions and a single assembly with full axial detail. The single assembly is used for the majority of sensitivity tests, particularly for the sensitivities to 3D parameters. While radial MOC parameters (such as radial mesh and radial ray spacing) have been previously studied in great detail for 2D MOC solvers~\cite{rhodes2006casmo}, the 3D MOC parameters (axial ray spacing, polar angles, and axial source height) have not been thoroughly examined. 

An accuracy criteria of less than 1.0\% Root Mean Square (RMS) and 3.0\% maximum pellet-wise fission rate error is used to determine the appropriate parameters where each pellet-wise fission rate is defined by the fission rate within a 2.0 cm zone of a fuel rod. An eigenvalue bias of less than 20 pcm is also required. The resulting MOC parameters are shown in Table~\ref{tab:final-params}.

\begin{table}[ht]
	\centering
	\caption{MOC ray and mesh parameters determined to accurately and efficiently simulate PWR problems}
	\medskip
	\begin{tabular}{lc}
		\hline
		Radial Ray Spacing & 0.05 cm \\
		Axial Ray Spacing & 0.75 cm \\
		Number of Azimuthal Angles & 64 \\
		Number of Polar Angles & 10 \\
		\hline
		Number of Fuel Sectors & 4 \\
		Number of Guide Tube Sectors & 8 \\
		Number of Moderator Sectors & 8 \\
		Axial Source Height & 2.0 cm \\
		Radial Reflector Mesh & $3\times 3$ cells per pin-cell mesh \\
		\hline
		CMFD Cell Height & 2.0 cm \\
		Number of CMFD Energy Groups & 8 \\
		\hline
	\end{tabular}
	\label{tab:final-params}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Full Core Results}


Using the MOC parameters presented in Table~\ref{tab:final-params}, the BEAVRS benchmark is simulated using the linear source 3D MOC solver in OpenMOC. The resulting radially and axially integrated reaction rates are presented in Figure~\ref{fig:full-core-radial} and Figure~\ref{fig:full-core-axial}.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/results/rr-plots/beavrs-3d-radial.png}
	\caption{Radial fission rate distribution for the BEAVRS benchmark formed by OpenMOC with reaction rates axially integrated.}
	\label{fig:full-core-radial}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/results/rr-plots/beavrs-3d-axial.png}
	\caption{Axial fission rate distribution for the BEAVRS benchmark formed by OpenMOC with reaction rates radially integrated.}
	\label{fig:full-core-axial}
\end{figure}


The OpenMOC results are compared with an OpenMC Monte Carlo solution from which the cross-sections were derived. The Monte Carlo simulation used 400 batches (300 inactive, 100 active) with $2 \times 10^8$ particles per batch. A comparison of the OpenMOC and OpenMC solutions is presented in Table~\ref{tab:openmc-comparison} in which fission rates are calculated on a pellet-wise scale (2.0 cm axial height).

\begin{table}[ht]
	\centering
	\caption{Simulation accuracy of OpenMOC relative to an OpenMC reference solution}
	\medskip
	\begin{tabular}{l|l|c|c|c}
		%\hline
		&                               & Avg. Fission Rate & RMS Fission & Max Fission \\
		& $k_{\textit{eff}}$ eigenvalue & Std. Dev.         & Rate Error & Rate Error \\
		\hline
		OpenMC  & 0.99927 +/- $1 \times 10^{-5}$  & 1.82\% & --     & -- \\
		OpenMOC & 0.99677                         & --     & 2.14\% & 7.52\% \\
		\hline
	\end{tabular}
	\label{tab:openmc-comparison}
\end{table}

Notice that since Monte Carlo simulations introduce, each pellet-wise Monte Carlo fission rate has an associated standard deviation. The RMS pellet-wise fission rate error of OpenMOC is 2.14\% which is close to the average standard deviation in OpenMC. To determine the shape of the error distribution, fission rate errors are integrated in the axial and radial directions. These radial and axial distributions are shown in Figure~\ref{fig:openmc-comp-rad} and Figure~\ref{fig:openmc-comp-ax}, respectively.


\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/results/full-core/radial_error_v_openmc.png}
	\caption{Radial distribution of relative fission rate errors (\%) of OpenMOC compared with a reference OpenMC solution on the BEAVRS benchmark.}
	\label{fig:openmc-comp-rad}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/results/full-core/axial_error_v_openmc.png}
	\caption{Axial distribution of relative fission rate errors (\%) of OpenMOC compared with a reference OpenMC solution on the BEAVRS benchmark.}
	\label{fig:openmc-comp-ax}
\end{figure}


To ensure the gross fission rate distribution is accurately simulated, assembly-integrated fission rates are compared. Figure~\ref{fig:assembly-rr} shows these assembly rate errors and the associated OpenMC reference folded into a 1/8 core map. Similar to the radial distribution shown in Figure~\ref{fig:openmc-comp-rad}, the error seems to be a pure tilt across the core. This tilt is likely due to the particular transport correction applied to the cross-sections not being able to fully capture the effects of anisotropic scattering. It is possible that a better transport correction -- particularly in reflector regions -- would be able to eliminate this tilt.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{figures/results/full-core/folded-1-8-core-assembly-rr.png}
	\caption{1/8 core folded assembly fission rate error of OpenMOC compared with the reference OpenMC solution for the BEAVRS benchmark.}
	\label{fig:assembly-rr}
\end{figure}

\newpage

The computational requirements of the full core solution in OpenMOC is presented in Table~\ref{tab:full-core-comp-req}. Note that while the number of core-hours required to converge the problem is high (717,465), the solution was computed on the Argonne BlueGene/Q supercomputer, which has extremely slow cores for energy efficiency reasons. The requirements on modern computing cores, such as those on the Falcon supercomputer, is estimated by comparing the integration time per core of the SDSA problem on the Falcon supercomputer (48.96 ns/core) to those on the Argonne BlueGene/Q supercomputer (175.36 ns/core) for one node using all available cores.

\begin{table}[ht]
	\centering
	\caption{Computational requirements of OpenMOC on the full core 3D BEAVRS benchmark using the Mira partition of the Argonne BlueGene/Q supercomputer}
	\medskip
	\begin{tabular}{l|l}
		\hline
		Runtime & 7.76 hours \\
		Number of Transport Sweeps & 20 \\
		Nodes & 5780 ($17 \times 17 \times 20$) \\
		CPU Cores & 92480 \\
		Integration Time per Core & 256.7 ns / core \\
		Computational Cost & 717,465 core-hours \\
		Estimated Computational Cost on Falcon & $\approx$ 200,314 core-hours \\
		\hline
	\end{tabular}
	\label{tab:full-core-comp-req}
\end{table}

Further analysis of the computational profile is presented in Table~\ref{tab:full-core-comp-prof}. The results show the computational overhead of CMFD acceleration was insignificant. In addition, the time spent communicating boundary angular fluxes between domains was quite small. However, there was significant idle time between sweeps when nodes are waiting for others to finish their current iteration. This is an indication of load imbalance due to more work required in core regions than in reflector regions.

\begin{table}[ht]
	\centering
	\caption{Computational profile of OpenMOC on the full core 3D BEAVRS benchmark using the Mira partition of the Argonne BlueGene/Q supercomputer}
	\medskip
	\begin{tabular}{lll|c|c}
		\hline
		& & & Computation & Fraction of \\
		\multicolumn{3}{c|}{Solver Component} & Time (s) & Runtime\\
		\hline
		Total & & & $2.79 \times 10^4$ & 100\% \\
		& Transport Sweeps & & $2.67 \times 10^4$ & 95.8\% \\
		& & Angular Flux Communication & $7.05 \times 10^2$ & 2.5\% \\
		& & Idle Time Between Sweeps & $7.74 \times 10^3$ & 27.7\% \\
		& CMFD Solver & & $97.4$ & 0.3\% \\		
		\hline
	\end{tabular}
	\label{tab:full-core-comp-prof}
\end{table}

\newpage

            
In order to verify that the chosen MOC parameters where sufficient in accurately converging the solution, simulations are conducted in which each 3D parameter is refined, with the results shown Table~\ref{tab:fc-param-sensitivity}. Ideally, all should be refined together, but the computational burden would be too great. Therefore, each parameter is refined separately. Due to memory constraints, ray parameters were only able to be refined by $\approx 1.5 \times$ but the axial mesh could be refined by a factor of 2. These results indicate that the solution nearly adheres to the required criteria of less than 1.0\% pellet-wise RMS fission rate error, less than 3.0\% maximum error, and less than 20 pcm bias. In order to strictly adhere to the criteria, the axial mesh would need to be slightly refined.

\begin{table}[ht]
	\centering
	\caption{Differences observed from refining 3D MOC parameters for the BEAVRS benchmark relative to the first solution}
	\medskip
	\begin{tabular}{l|l|l|c|c|c}
		\hline
		Polar  & Axial Ray & Source & $k_{\textit{eff}}$  & RMS Fission & Max Fission \\
		Angles & Spacing   & Height & Bias                & Rate Diff. & Rate Diff. \\
		\hline
		10 & 0.75 cm & 2.0 cm & --     & --     & --  \\
		14 & 0.75 cm & 2.0 cm & 2 pcm  & 0.51\% & 1.92\%  \\
		10 & 0.50 cm & 2.0 cm & 5 pcm  & 0.41\% & 1.76\%  \\
		10 & 0.75 cm & 1.0 cm & 13 pcm & 0.28\% & 3.33\%  \\
		\hline
	\end{tabular}
	\label{tab:fc-param-sensitivity}
\end{table}


\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Conclusions}

The main goal of this thesis was to develop a 3D MOC solver capable of accurately simulating the BEAVRS benchmark. This goals was motivated by the desire to develop methods which can explicitly handle axial as well as radial variation within PWR reactor cores. The following summarizes the key accomplishments demonstrated by this thesis to meet this challenge:

\begin{itemize}

\item \textbf{Re-design of the OpenMOC software infrastructure.} The original OpenMOC software infrastructure was relatively inflexible, designed for one particular implementation of a 2D MOC solver. The internal structure was altered to be modular and flexible, accommodating multiple ray tracing and solver schemes.

\item \textbf{The implementation of an efficient 3D MOC solver.} A 3D MOC solver was implemented in OpenMOC which is capable of efficiently solving the MOC equations using an efficient track laydown, on-the-fly ray tracing, and spatial domain decomposition. In addition, an efficient linear source solver was added, allowing for accurate solutions with relatively coarse mesh.

\item \textbf{Theoretical and practical evaluation of source iteration convergence.} The convergence of transport codes using source iteration (such as MOC) with transport-corrected cross-sections has plagued researchers in the past. In this thesis, a robust theoretical framework is introduced to understand convergence characteristics. The diagonal stabilization scheme was presented which alleviates the convergence issues.

\item \textbf{3D MOC parameter refinement studies.} Since previous 3D MOC solvers have not been capable of solving large PWR problems, parameter refinement studies have not been fully explored. In this thesis, the sensitivity of solution accuracy to each 3D MOC parameter is thoroughly explored.

\item \textbf{Evaluation of computational requirements for solving full core PWR problems.} Since OpenMOC is the first solver capable of solving the BEAVRS benchmark with deterministic methods, it provides a useful indicator for the computational cost of solving such a large problem.

\end{itemize}

Past 3D deterministic solvers have not been capable of fully resolving full core PWR models to pellet-level precision. This thesis shows that these large scale simulations are now possible with careful consideration of implementation details critical to performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Future Work}

This thesis was able to reasonably simulate the BEAVRS benchmark. However, there was a noticeable tilt across the core, likely due to the transport correction not properly accounting for anisotropic scattering. Therefore, this thesis illuminates the need for a better transport correction, particularly in reflector regions where the traditional in-scatter transport correction might not be sufficient. With an improved transport correction, the simulation results could be made more accurate with the same computational cost.

Future work in OpenMOC should also concentrate on reducing computational cost. Using the current solver, only uniform mesh refinement was studied for the axial direction in this thesis. A finer mesh could be used near reflector regions with a coarser mesh in the central core to improve accuracy and decrease computational cost.

Algorithmic aspects of the OpenMOC domain decomposition could be improved. For instance, the requirement of uniform spatial domain decomposition leads to load balancing inefficiencies, as observed on the BEAVRS benchmark. If domains could be merged, this issue might be alleviated. Additionally, OpenMOC currently requires double storage of boundary angular fluxes so that information is not overwritten during their exchange between nodes. However, it might be possible to only store the information once if a clever algorithm is implemented to prevent overwriting of information. Since the memory usage is dominated by boundary angular flux storage, this would reduce the overall memory requirements by nearly a factor of two.

The source approximations could also be studied in greater detail. Currently, a single source approximation (either flat or linear) is used for all regions in the core during a single simulation. However, in the modular framework, it is possible to create a solver which mixes flat and linear source approximations. For instance, moderator regions could always be simulated with a linear source approximation where there is a significant gradient, but gap and clad regions could be simulated with a flat source approximation where the neutron source is quite small. Additionally, source approximations restricted to only the axial direction, such as linear or quadratic, could be implemented for regions where radial variation is not significant.

In addition to implementing different source approximations, it would be useful to store angular and spatial dependent cross-sections. Other authors have found a bias introduced by not accounting for the angular dependence of multi-group cross-sections~\cite{gibson-preprint}. Therefore, this should be treated in order to develop more accurate simulations capable of matching a fully converged continuous energy Monte Carlo solution. Also, spatial dependence of cross-sections would be useful for depletion analysis. In current methodologies, fuel is discretized into many regions in order to account for burnup gradients. If the variation could be captured with a spatially-dependent cross-section approximation, coarser mesh could allow for decreased computational cost of depletion studies.

An important issue studied in this thesis was the convergence behavior of source iteration with transport-corrected cross-sections. However, the theoretical discussion of source iteration presented in this thesis relied on a flat source approximation without CMFD acceleration. The diagonal stabilization scheme was shown to also work for CMFD accelerated cases as well as the linear source solver, but a theoretical study would be useful, perhaps leading to an improved stabilization strategy.

Other improvements should be made to OpenMOC to improve its usability. First, the domain decomposed CMFD solver should be extended to allow splitting of CMFD cells across domain boundaries. This would allow the domain decomposition strategy to be less rigid. The current treatment is not very flexible for the BEAVRS benchmark in which assemblies contain a $17 \times 17$ lattice of pins. Since pin-cell CMFD mesh is standard, this imposes limitations on how the assembly can be domain decomposed. Additionally, the CMFD solver should be extended to include other linear solvers. Currently, only a red-black SOR linear solver is available, which has a limited stability region. A more general linear solver, such as GMRES, would allow for a fall-back if the red-black SOR linear solver fails to converge. Lastly, the current coordinate data structure is not well designed. While it is only used for 2D ray tracing, which is not a significant computational cost, it could be greatly improved by changing from a linked-list representation to a vector representation.

Finally, the development of 3D transport methods should focus on making high fidelity reactor simulations feasible. The results presented in this thesis used many-group cross-section libraries and solution of the BEAVRS benchmark required a large supercomputer. These many-group cross-section libraries were used to reduce the spatial variation of cross-sections such that they are only dependent on the material, not the spatial location. However, PWR simulations would be far more feasible if region-dependent several-group cross-sections were capable of accurately capturing neutron behavior. Therefore, future analysis should investigate several-group cross-section formations which maintain solution accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY

\begin{singlespace}
\bibliographystyle{ans}
\bibliography{references}
\end{singlespace}

\end{document}
