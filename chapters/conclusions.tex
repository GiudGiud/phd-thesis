\chapter{Conclusions}
\label{chap:conclusions}

With a desire to move to more detailed models, Monte Carlo is becoming
competitive with deterministic methods for high fidelity neutronics simulations
of nuclear reactor systems. This is particularly true in light of the inherent
parallel nature of the algorithm, which fits well with increasing
concurrency of current and predicted high performance computing architectures.
In addition, the physics can be treated with much greater accuracy than with
deterministic methods, and the geometry of the problem can be treated exactly.
However, the method requires a vast number of particle histories in order to
achieve adequate statistical convergence over a fine tally mesh, necessitating an
immense computational cost. This should be addressable with the scale of HPC
machines, but only if the parallel model can successfully deal with the
considerable memory burden imposed by the number of material densities and tally
results needed for robust analyses.

This memory burden can be handled with domain decomposition. However, until
recently it was widely-expected that such an approach was not viable due to
particle communication costs and load imbalances. This work aimed to add to the
body of work that contests this thinking, by performing a real analysis with a
real domain decomposition implementation.

Chapter 1 mentioned several issues: particle communication and load balancing,
memory treatments for materials and tallies, and practicalities of carrying out
real analyses. In this chapter we will discuss the progress achieved by this
thesis, as well as future work that remains.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Particle Communication and Load Balancing}

The two primary issues thought to impede the use of domain decomposition were
particle communication costs and load imbalances \cite{forrest_mc_prospects,
martin_chall}. The overhead from these two aspects were previously observed to
be significant even for small particle workloads as described first by Alme et
al. \cite{js-alme-2001} in 2001, and subsequent works tended to look for ways
to reduce it, \emph{e.g.}, with overlapping domains as described by Wagner et
al.\cite{wagner_dd} or with more complicated asynchronous schemes such as those
described by Brunner et al. \cite{Brunner20093882}.

Recent works by Siegel et al. and Romano showed that communication costs were
much less than conventional wisdom indicated for real nuclear reactor analysis.
Their studies made this claim using a performance model based on leakage rates
tallied from a real core model. However, true validation requires a real
implementation at scale.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Contributions of Present Work}

In Chapter 2, a complete implementation of domain decomposition was discussed.
This included a number of intricacies and practicalities needed to actually make
it work, and make it work well. This eschewed the more complicated asynchronous
schemes discussed by Brenner et al. in the literature, opting for what was used
by Siegel et al. to add a series of blocking particle synchronization stages to
the algorithm. At the core was the particle communication algorithms, which
ensure scalability by fixing the number of domains processes would need to
communicate with between stages, and at the same time facilitating efficient
load balancing with the resource matching routines. In addition, Chapter 2
describes the numerous details required for random number reproducibility,
which guarantees identical particle tracking by communicating random number seeds
with particles across domain boundaries. Modifications to make the initial
source site and inter-batch fission site sampling routines reproducible are also
discussed.

Chapters 3 and 4 demonstrated the performance of the particle communication and
load-balancing algorithms, confirming the efficiency for realistic particle
tracking loads over the BEAVRS \ac{PWR} problem. Chapter 3 also
updated the theoretical framework established by Siegel et al. and Romano to
better predict load imbalance performance from particle tracking data with fewer
approximations. This updated model was used to reprocess data tallied from fine
domain meshes of the previous studies to show that performance degradation was
significantly over-estimated. Similarly, Chapter 4 describes the effectiveness
of the resource-matching load balancing strategy in the same performance model
terms, and in that context presents an algorithm to optimize the process
distribution across domains and minimize load imbalances.

Ultimately, this work showed that with several relatively-easy load balancing
strategies, domain decomposition can be done with Monte Carlo on \ac{PWR}
problems with slowdowns over non-decomposed runs of between 1.4x to 1.75x.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Future Work}

Though the current work demonstrates only modest overhead for particle
communication and load imbalances that can be largely mitigated, there are a
number of extra optimizations that can be pursued to improve the situation
further. First, the existing optimizations to particle communication discussed
in the literature can be implemented, such as overlapping domains and
asynchronous inter-domain particle transfers, However, given the good
performance of the current method it is unclear whether or not the additional
complexity is worth any speedups achieved. In a similar vein, more nuanced
particle buffering can be explored, such as the sparse and in-place buffering
schemes discussed by Felker et al. in \cite{Felker04062012}.

Additional contributions can be made to the load balancing situation. For
instance, performance could be improved by implementing more general unstructured
meshes to better match the mesh to the load, and automatic sizing and/or
re-balancing during simulation might pursued in conjunction, such as discussed by
Procassini et al. \cite{Procassini}. However, automatic mesh refinement and
re-balancing must deal with the difficulty of moving tally and material data
between compute nodes when they are reassigned, which is likely to incur
significant overhead. In any case, more general meshing capabilities will be
needed to avoid cut cells in different reactor types, such as with hexagonal
geometries commonly found in fast reactors. In doing so, care must be taken to
ensure equivalent treatment with the communication stencil, where processes only
communicate among the local neighborhood.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Memory Treatment}

The scope of the full-core problem presented a challenge, with terabytes of
aggregate memory needed for a faithful treatment of depletion. This was an issue
for Monte Carlo mainly because in the traditional parallel model the entire
problem needed to be replicated across all compute nodes: a situation for which
some form of decomposition is needed if Monte Carlo is to have any hope of
tackling the problem. Previous work by Romano \cite{romano_thesis} has predicted
that data decomposition is promising avenue for dealing with tally memory, but
it is still unclear whether or not this would be efficient for treating nuclide
atom densities in each material, which requires over 100 gigabytes over the
whole problem. Furthermore, such a method has yet to be demonstrated at the full
scale of the problem with a full implementation.

Domain decomposition promises to alleviate both tally and material memory
issues, but several practicalities of memory management still presented a
challenge. Specifically, the machinery of loading domain-specific quantities
into memory needs to be demonstrated at scale, and it needs to be done in a way
that doesn't unsustainably grow the number of domains or significantly degrade
particle tracking rates. This is made complicated by the necessity of tally
reduction within a domain when using the resource matching load balancing
strategy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Contributions of Present Work}

In Chapter 2, an on-the-fly (OTF) memory treatment was described that enables
memory for domain-specific quantities to be allocated only when needed as a
result of particles tracking through the corresponding regions of the problem.
This allows us to avoid the potentially complicated task of automatic
determination of domain residence for arbitrary tally regions in arbitrary
domain meshes, and removes responsibility for the task from users. Algorithms
are also presented to efficiently carry out tally reduction using OTF-allocated
tally results, which requires a sort after the first batch of particles to
utilize the same contiguous-block memory operations for tally reduction that
were previously developed by Romano for efficient tally performance
\cite{romano_thesis}. This was done in an efficient manner, scaling by the
number of tallies in a domain as O($n$) in time and O(1) in memory.

Results in Chapter 5 demonstrated the effectiveness of these treatments to
handle OTF loading of materials and tallies at the full scale of the problem,
demonstrating that any additional overhead generated by the method does not
degrade domain decomposition performance or impede load balancing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Future Work}

Though the memory treatment in the current implementation was shown to
successfully enable full-scale domain-decomposed runs with the true memory
burden needed for high-fidelity simulations, several drawbacks are apparent that
could use improvements.

First, as discussed in Section~\ref{sec:tal_reduction_limits}, the current
implementation does not robustly treat tally cells that are cut by domain
boundaries, instead treating the sections that fall in different domains as
unique tallies independent of one another. This could lead to problems with high
aspect ratio (``slivers''), volume calculation difficulties for each section, and
the fact that subsequent depletion steps would change the materials of a cell in a
manner that is dependent on the domain mesh. Thus with the current
implementation this situation must be avoided. However, if a scheme could be
devised to reduce these tallies across domain boundaries, cut cells could be used
without issue. This might remove the need to implement more general domain
meshing, which might be very attractive under certain circumstances. The
difficulty here lies in finding a way to do this efficiently, since in this case
a tally might potentially (although highly unlikely) need to be reduced amongst
processes on \emph{every} other domain, which could cripple parallel
scalability. Furthermore, the meta-data required to carry this out threatens to
grow exponentially, since every scoring bin might need to maintain a list of all
other domains the quantity needs to be reduced with.

Another optimization that could be pursued is a form of ``memory load
balancing,'' whereby data decomposition is used to normalize the memory burden
across processes. For instance, as discussed in Chapter 5 there were numerous
domains around the periphery that did not overlap with any tally or material
data, yet still contained appreciable particle tracking workloads. With the data
decomposition scheme discussed by Romano et al. \cite{Romano201320,
romano_talserver_paris}, tally results might be stored on processes assigned to
these domains. Furthermore, such a scheme might also be used to handle
cut-cells, though the issue of meta-data and overhead must still be addressed
for this to be done efficiently at scale.

Finally, since memory is allocated in an OTF fashion, the ordering in memory on
any particular domain is scrambled. Currently this means that maps must be used
in post-processing to unscramble these arrays if results are to be matched with
specific locations for analysis or multi-physics coupling. Attempts were made in
this work to do this unscrambling in OpenMC during parallel I/O in finalization,
but performance suffered significantly when writing to different sections of
ordered data files (on the Lustre filesystem on Titan, $\sim 60$ gigabytes per
second when dumping contiguous chunks to separate files vs $\sim 100$ megabytes
per second when writing individual values in the proper positions of a single
output file). Solving this issue is mainly a matter of convenience for the user,
but it could have performance implications during iterative multi-physics
coupling schemes in the future.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Full Core Analysis}

Though previous works have attempted to solve the PWR problem according to the
specifications of Smith's first challenge in 2003 using Monte Carlo, none have
demonstrated the complete tally and material memory burden imposed by the
detailed tally mesh specified by Smith and Forget's subsequent challenge in
2013. The last objective of this thesis was to use the complete domain
decomposition implementation that was added to OpenMC to carry out this kind of
analysis and characterize the performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Contributions of Present Work}

In Chapter 5, the full scale PWR depletion problem was addressed with the new
domain decomposition implementation using OTF tally and memory treatment. This
was successfully carried out with 10 rings and 100 axial sections per fuel pin
in the BEAVRS PWR benchmark in the finest case, with 6 reaction rate tallies
for each of 206 nuclides in each region. This constituted an aggregate of 79
gigabytes of memory for material atom densities, and 1.4 terabytes of memory for
tallies. Particle tracking rates were observed to be comparable with non-DD
runs, with only modest slowdowns observed during active batch tallies as a
result of increased tally complexity (and which have nothing to do with the
number of domains used or the number of particle histories run). These specific
runs were small, with the largest running only 5 batches of 2.45 million
particles per batch, but the particle tracking rates allowed for an estimation
of the computational cost that would be needed for longer production runs. For
example, a run with the same tally mesh that tracks 10 billion particle
histories should take 8.3 CPU-years on the Titan Cray XK7 supercomputer. This
amounts to just over 2 million core-hours on the same machine, which is
manageable given the size of typical allocations on HPC machines. With the
incorporation of shared memory parallelism, the use of load balancing, and
optimized compilation, this run would be achievable on 10,000 compute nodes in
just over 7 hours.

Through this analysis, insight into the practicalities of the problem
specification was also achieved, and several heuristics were developed to help
with the choice of domain decomposition parameters relevant to reactor problems.
Specifically, a domain mesh that contained assemblies in 3x3 radial groupings
was determined to be ideal to avoid cut cells, with which axial refinements of
up to 50 domains were used to accommodate the full tally memory burden arising
from depletion tallies over 10 radial rings and 100 axial sections in each fuel
pin.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Future Work}

As discussed in Chapter 5, the overall performance of large-scale analyses
could be improved with more optimized tally scoring algorithms to increase
particle tracking rates during active batches. Importantly, this includes the
incorporation of shared memory parallelism, which shouldn't conflict at all with
this domain decomposition implementation as long as only one ``master thread''
per MPI process is used for inter-domain particle communication and intra-domain
tally reduction. This might require a few extra data reductions among threads.

In addition, a more sophisticated suite of post-processing tools should be
developed to handle tally results at this scale. This is needed for analysis of
results as well as coupling to other multi-physics routines. Importantly, these
tools will also need to operate in a parallel and memory-efficient manner, since
of course the results are not expected to fit into memory on any single compute
node that will carry out such processing.

Finally, future work might continue to scale the problem up with finer
depletion meshes, more nuclides, and more reaction rate tallies per nuclide in
each region. Performance is not expected to suffer due to the domain
decomposition implementation in these cases, except of course if domains must
become exceedingly small to accommodate extreme memory burdens.

